{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from deeppavlov.core.common.registry import register\n",
    "from deeppavlov.core.models.tf_model import TFModel\n",
    "from deeppavlov.models.squad.utils import dot_attention, simple_attention, PtrNet, CudnnGRU, CudnnCompatibleGRU\n",
    "from deeppavlov.core.common.check_gpu import GPU_AVAILABLE\n",
    "from deeppavlov.core.layers.tf_layers import cudnn_bi_gru, variational_dropout\n",
    "from deeppavlov.core.common.log import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "@register('squad_model')\n",
    "class SquadModel(TFModel):\n",
    "    \"\"\"\n",
    "    SquadModel predicts answer start and end position in given context by given question.\n",
    "\n",
    "    High level architecture:\n",
    "    Word embeddings -> Contextual embeddings -> Question-Context Attention -> Self-attention -> Pointer Network\n",
    "\n",
    "    Parameters:\n",
    "        word_emb: pretrained word embeddings\n",
    "        char_emb: pretrained char embeddings\n",
    "        context_limit: max context length in tokens\n",
    "        question_limit: max question length in tokens\n",
    "        char_limit: max number of characters in token\n",
    "        char_hidden_size: hidden size of charRNN\n",
    "        encoder_hidden_size: hidden size of encoder RNN\n",
    "        attention_hidden_size: size of projection layer in attention\n",
    "        keep_prob: dropout keep probability\n",
    "        learning_rate: initial learning rate\n",
    "        min_learning_rate: min learning rate, is used in learning rate decay\n",
    "        learning_rate_patience: number of epochs without score improvements to decay learning rate\n",
    "        grad_clip: gradient clipping value\n",
    "        weight_decay: weight decay value\n",
    "    \"\"\"\n",
    "    def __init__(self, word_emb: np.ndarray, char_emb: np.ndarray, context_limit: int = 450, question_limit: int = 150,\n",
    "                 char_limit: int = 16, train_char_emb: bool = True, char_hidden_size: int = 100,\n",
    "                 encoder_hidden_size: int = 75, attention_hidden_size: int = 75, keep_prob: float = 0.7,\n",
    "                 learning_rate: float = 0.5, min_learning_rate: float = 0.001, learning_rate_patience: int = 1,\n",
    "                 grad_clip: float = 5.0, weight_decay: float = 1.0, **kwargs):\n",
    "\n",
    "        self.init_word_emb = word_emb\n",
    "        self.init_char_emb = char_emb\n",
    "        self.context_limit = context_limit\n",
    "        self.question_limit = question_limit\n",
    "        self.char_limit = char_limit\n",
    "        self.train_char_emb = train_char_emb\n",
    "        self.char_hidden_size = char_hidden_size\n",
    "        self.hidden_size = encoder_hidden_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.learning_rate_patience = learning_rate_patience\n",
    "        self.grad_clip = grad_clip\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.word_emb_dim = self.init_word_emb.shape[1]\n",
    "        self.char_emb_dim = self.init_char_emb.shape[1]\n",
    "\n",
    "        self.last_impatience = 0\n",
    "        self.lr_impatience = 0\n",
    "\n",
    "        if GPU_AVAILABLE:\n",
    "            self.GRU = CudnnGRU\n",
    "        else:  \n",
    "            # you can't run cudnn layers on cpuï¼Œ this cudnn compatible makes possible to load model trained on gpu and run later on cpu\n",
    "            # normal GRUCell can be used on both cpu and gpu but slower\n",
    "            self.GRU = CudnnCompatibleGRU  \n",
    "\n",
    "        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        self.sess_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=self.sess_config)\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "        self._init_optimizer()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        # Try to load the model (if there are some model files the model will be loaded from them)\n",
    "        if self.load_path is not None:\n",
    "            self.load()\n",
    "            if self.weight_decay < 1.0:\n",
    "                 self.sess.run(self.assign_vars)\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self._init_placeholders()\n",
    "\n",
    "        self.word_emb = tf.get_variable(\"word_emb\", initializer=tf.constant(self.init_word_emb, dtype=tf.float32),  # pretrained word embedding\n",
    "                                        trainable=False)  \n",
    "        self.char_emb = tf.get_variable(\"char_emb\", initializer=tf.constant(self.init_char_emb, dtype=tf.float32),  # pretrained char embedding\n",
    "                                        trainable=self.train_char_emb)  # char_emb trainable\n",
    "\n",
    "        self.c_mask = tf.cast(self.c_ph, tf.bool)  # mask for padding, used in the pointer network\n",
    "        self.q_mask = tf.cast(self.q_ph, tf.bool)\n",
    "        self.c_len = tf.reduce_sum(tf.cast(self.c_mask, tf.int32), axis=1)  # actual word length for context\n",
    "        self.q_len = tf.reduce_sum(tf.cast(self.q_mask, tf.int32), axis=1)  # actual word length for question\n",
    "\n",
    "        bs = tf.shape(self.c_ph)[0]  # batch_size\n",
    "        self.c_maxlen = tf.reduce_max(self.c_len)   # max word length for context\n",
    "        self.q_maxlen = tf.reduce_max(self.q_len)   # max word lenght for question\n",
    "        self.c = tf.slice(self.c_ph, [0, 0], [bs, self.c_maxlen])  # [bs, c_maxlen]\n",
    "        self.q = tf.slice(self.q_ph, [0, 0], [bs, self.q_maxlen])  # [bs, q_maxlen]\n",
    "        self.c_mask = tf.slice(self.c_mask, [0, 0], [bs, self.c_maxlen])\n",
    "        self.q_mask = tf.slice(self.q_mask, [0, 0], [bs, self.q_maxlen])\n",
    "        self.cc = tf.slice(self.cc_ph, [0, 0, 0], [bs, self.c_maxlen, self.char_limit])  # [bs, c_maxlen, char_limit]\n",
    "        self.qc = tf.slice(self.qc_ph, [0, 0, 0], [bs, self.q_maxlen, self.char_limit])  # [bs, q_maxlen, char_limit]\n",
    "        self.cc_len = tf.reshape(tf.reduce_sum(tf.cast(tf.cast(self.cc, tf.bool), tf.int32), axis=2), [-1])  # actual char length for context, reshape to 1D tensor\n",
    "        self.qc_len = tf.reshape(tf.reduce_sum(tf.cast(tf.cast(self.qc, tf.bool), tf.int32), axis=2), [-1])  # actual char length for question, reshape to 1D tensor\n",
    "        # to remove char sequences with len equal zero (padded tokens); token padding's char padding, have exceptions run on CPU\n",
    "        self.cc_len = tf.maximum(tf.ones_like(self.cc_len), self.cc_len)\n",
    "        self.qc_len = tf.maximum(tf.ones_like(self.qc_len), self.qc_len)\n",
    "        # one-hot encoding target\n",
    "        self.y1 = tf.one_hot(self.y1_ph, depth=self.context_limit)  # [batch_size, context_limit]\n",
    "        self.y2 = tf.one_hot(self.y2_ph, depth=self.context_limit)\n",
    "        self.y1 = tf.slice(self.y1, [0, 0], [bs, self.c_maxlen])   # [batch_size, c_maxlen]\n",
    "        self.y2 = tf.slice(self.y2, [0, 0], [bs, self.c_maxlen])\n",
    "\n",
    "        with tf.variable_scope(\"emb\"):\n",
    "            # char embedding\n",
    "            with tf.variable_scope(\"char\"):  \n",
    "                # char-embedding\n",
    "                cc_emb = tf.reshape(tf.nn.embedding_lookup(self.char_emb, self.cc),\n",
    "                                    [bs * self.c_maxlen, self.char_limit, self.char_emb_dim])  # [bs * self.c_maxlen, self.char_limit, self.char_emb_dim]\n",
    "                qc_emb = tf.reshape(tf.nn.embedding_lookup(self.char_emb, self.qc),\n",
    "                                    [bs * self.q_maxlen, self.char_limit, self.char_emb_dim])  # [bs * self.q_maxlen, self.char_limit, self.char_emb_dim]\n",
    "\n",
    "                # variational dropout\n",
    "                # same drouput mask for each timestep\n",
    "                cc_emb = variational_dropout(cc_emb, keep_prob=self.keep_prob_ph)  # [bs * self.c_maxlen, self.char_limit, self.char_emb_dim]\n",
    "                qc_emb = variational_dropout(qc_emb, keep_prob=self.keep_prob_ph)  # [bs * self.q_maxlen, self.char_limit, self.char_emb_dim]\n",
    "\n",
    "                # bi_gru\n",
    "                _, (state_fw, state_bw) = cudnn_bi_gru(cc_emb, self.char_hidden_size, seq_lengths=self.cc_len,\n",
    "                                                       trainable_initial_states=True)  # char-encoding on context\n",
    "                cc_emb = tf.concat([state_fw, state_bw], axis=1)\n",
    "\n",
    "                _, (state_fw, state_bw) = cudnn_bi_gru(qc_emb, self.char_hidden_size, seq_lengths=self.qc_len,\n",
    "                                                       trainable_initial_states=True,\n",
    "                                                       reuse=True)  # reuse weights for question char-encoding\n",
    "                qc_emb = tf.concat([state_fw, state_bw], axis=1)\n",
    "\n",
    "                cc_emb = tf.reshape(cc_emb, [bs, self.c_maxlen, 2 * self.char_hidden_size])  # [bs, c_maxlen, 2*char_hidden_size]\n",
    "                qc_emb = tf.reshape(qc_emb, [bs, self.q_maxlen, 2 * self.char_hidden_size])  # [bs, q_maxlen, 2*char_hidden_size]\n",
    "\n",
    "            # word embedding\n",
    "            with tf.name_scope(\"word\"):\n",
    "                c_emb = tf.nn.embedding_lookup(self.word_emb, self.c)  # [bs, c_maxlen, word_emb_dim]\n",
    "                q_emb = tf.nn.embedding_lookup(self.word_emb, self.q)  # [bs, q_maxlen, word_emb_dim]\n",
    "\n",
    "            c_emb = tf.concat([c_emb, cc_emb], axis=2)  # [bs, c_maxlen, word_emb_dim + char_emb_dim]\n",
    "            q_emb = tf.concat([q_emb, qc_emb], axis=2)  # [bs, q_maxlen, word_emb_dim + char_emb_dim]\n",
    "\n",
    "        # GRU encoder to build representation for questions and passages, here we reuse/share weights\n",
    "        # we stack outputs from each gru layer as output\n",
    "        with tf.variable_scope(\"encoding\"):\n",
    "            rnn = self.GRU(num_layers=3, num_units=self.hidden_size, batch_size=bs,\n",
    "                           input_size=c_emb.get_shape().as_list()[-1],\n",
    "                           keep_prob=self.keep_prob_ph)\n",
    "            c = rnn(c_emb, seq_len=self.c_len)  # [bs, c_maxlen, hidden_size]\n",
    "            q = rnn(q_emb, seq_len=self.q_len)  # [bs, q_maxlen, hidden_size]\n",
    "\n",
    "        # match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation\n",
    "        # we use scaled dot-product attention from Attention is all you need paper\n",
    "        with tf.variable_scope(\"attention\"):\n",
    "            qc_att = dot_attention(c, q, mask=self.q_mask, att_size=self.attention_hidden_size,  \n",
    "                                   keep_prob=self.keep_prob_ph)   # [bs, c_maxlen, 2 * hidden_size]\n",
    "            rnn = self.GRU(num_layers=1, num_units=self.hidden_size, batch_size=bs,\n",
    "                           input_size=qc_att.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n",
    "            att = rnn(qc_att, seq_len=self.c_len)  # [bs, c_maxlen, hidden_size]\n",
    "\n",
    "        # self-matching attention layer\n",
    "        with tf.variable_scope(\"match\"):\n",
    "            self_att = dot_attention(att, att, mask=self.c_mask, att_size=self.attention_hidden_size,\n",
    "                                     keep_prob=self.keep_prob_ph)  # [bs, c_maxlen, 2 * hidden_size]\n",
    "            rnn = self.GRU(num_layers=1, num_units=self.hidden_size, batch_size=bs,\n",
    "                           input_size=self_att.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n",
    "            match = rnn(self_att, seq_len=self.c_len)  # [bs, c_maxlen, hidden_size]\n",
    "\n",
    "        # pointer network to locate the positions of answer from the passage\n",
    "        with tf.variable_scope(\"pointer\"):\n",
    "            init = simple_attention(q, self.hidden_size, mask=self.q_mask, keep_prob=self.keep_prob_ph)\n",
    "            pointer = PtrNet(cell_size=init.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n",
    "            logits1, logits2 = pointer(init, match, self.hidden_size, self.c_mask)\n",
    "\n",
    "        # loss\n",
    "        with tf.variable_scope(\"predict\"):\n",
    "            outer_logits = tf.exp(tf.expand_dims(logits1, axis=2) + tf.expand_dims(logits2, axis=1))\n",
    "            outer = tf.matmul(tf.expand_dims(tf.nn.softmax(logits1), axis=2),\n",
    "                              tf.expand_dims(tf.nn.softmax(logits2), axis=1))\n",
    "            outer = tf.matrix_band_part(outer, 0, tf.cast(tf.minimum(15, self.c_maxlen), tf.int64))\n",
    "            self.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n",
    "            self.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n",
    "            self.yp_logits = tf.reduce_max(tf.reduce_max(outer_logits, axis=2), axis=1)\n",
    "            loss_1 = tf.nn.softmax_cross_entropy_with_logits(logits=logits1, labels=self.y1)  # padding are included in the loss\n",
    "            loss_2 = tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=self.y2)\n",
    "            self.loss = tf.reduce_mean(loss_1 + loss_2)\n",
    "\n",
    "        # exponentially moving average of weight; used only during predicting\n",
    "        # when prediction, using the exp. moving average of weights of all steps\n",
    "        # this helps generalization\n",
    "        # in the deeppavlov 0.1 release, this chunck is removed, b/c its effect is small\n",
    "        if self.weight_decay < 1.0:\n",
    "            self.var_ema = tf.train.ExponentialMovingAverage(self.weight_decay)\n",
    "            ema_op = self.var_ema.apply(tf.trainable_variables())\n",
    "            with tf.control_dependencies([ema_op]):\n",
    "                self.loss = tf.identity(self.loss)\n",
    "\n",
    "                self.shadow_vars = []\n",
    "                self.global_vars = []\n",
    "                for var in tf.global_variables():\n",
    "                    v = self.var_ema.average(var)\n",
    "                    if v:\n",
    "                        self.shadow_vars.append(v)\n",
    "                        self.global_vars.append(var)\n",
    "                self.assign_vars = []\n",
    "                for g, v in zip(self.global_vars, self.shadow_vars):\n",
    "                    self.assign_vars.append(tf.assign(g, v))\n",
    "\n",
    "    def _init_placeholders(self):\n",
    "        self.c_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='c_ph')                     # context placeholder word-level: [batch_size, max_length_word]\n",
    "        self.cc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name='cc_ph')  # context placeholder char-level: [batch_size, max_length_word, max_legnth_char]\n",
    "        self.q_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='q_ph')                     # question placeholder word-level\n",
    "        self.qc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name='qc_ph')  # question placeholder char-level\n",
    "        self.y1_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y1_ph')                       # start position: [batch_size]\n",
    "        self.y2_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y2_ph')                       # end position: [batch_size]\n",
    "\n",
    "        self.lr_ph = tf.placeholder(dtype=tf.float32, shape=[], name='lr_ph')                           # learning rate placeholder\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')             # keep probability, default value is 1\n",
    "        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')             # \n",
    "\n",
    "    def _init_optimizer(self):\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                               initializer=tf.constant_initializer(0), trainable=False)\n",
    "            self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.lr_ph, epsilon=1e-6)\n",
    "            grads = self.opt.compute_gradients(self.loss)\n",
    "            gradients, variables = zip(*grads)\n",
    "            capped_grads = [tf.clip_by_norm(g, self.grad_clip) for g in gradients]\n",
    "            self.train_op = self.opt.apply_gradients(zip(capped_grads, variables), global_step=self.global_step)\n",
    "\n",
    "    def _build_feed_dict(self, c_tokens, c_chars, q_tokens, q_chars, y1=None, y2=None):\n",
    "        feed_dict = {\n",
    "            self.c_ph: c_tokens,\n",
    "            self.cc_ph: c_chars,\n",
    "            self.q_ph: q_tokens,\n",
    "            self.qc_ph: q_chars,\n",
    "        }\n",
    "        if y1 is not None and y2 is not None:\n",
    "            feed_dict.update({\n",
    "                self.y1_ph: y1,\n",
    "                self.y2_ph: y2,\n",
    "                self.lr_ph: self.learning_rate,\n",
    "                self.keep_prob_ph: self.keep_prob,\n",
    "                self.is_train_ph: True,\n",
    "            })\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def train_on_batch(self, c_tokens: np.ndarray, c_chars: np.ndarray, q_tokens: np.ndarray, q_chars: np.ndarray,\n",
    "                       y1s: Tuple[List[int], ...], y2s: Tuple[List[int], ...]) -> float:\n",
    "        \"\"\"\n",
    "        This method is called by trainer to make one training step on one batch.\n",
    "\n",
    "        Args:\n",
    "            c_tokens: batch of tokenized contexts\n",
    "            c_chars: batch of tokenized contexts, each token split on chars\n",
    "            q_tokens: batch of tokenized questions\n",
    "            q_chars: batch of tokenized questions, each token split on chars\n",
    "            y1s: batch of ground truth answer start positions\n",
    "            y2s: batch of ground truth answer end positions\n",
    "\n",
    "        Returns:\n",
    "            value of loss function on batch\n",
    "        \"\"\"\n",
    "        # TODO: filter examples in batches with answer position greater self.context_limit\n",
    "        # select one answer from list of correct answers\n",
    "        y1s = list(map(lambda x: x[0], y1s))\n",
    "        y2s = list(map(lambda x: x[0], y2s))\n",
    "        feed_dict = self._build_feed_dict(c_tokens, c_chars, q_tokens, q_chars, y1s, y2s)\n",
    "        loss, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def __call__(self, c_tokens: np.ndarray, c_chars: np.ndarray, q_tokens: np.ndarray, q_chars: np.ndarray,\n",
    "                 *args, **kwargs) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
    "        \"\"\"\n",
    "        Predicts answer start and end positions by given context and question.\n",
    "\n",
    "        Args:\n",
    "            c_tokens: batch of tokenized contexts\n",
    "            c_chars: batch of tokenized contexts, each token split on chars\n",
    "            q_tokens: batch of tokenized questions\n",
    "            q_chars: batch of tokenized questions, each token split on chars\n",
    "\n",
    "        Returns:\n",
    "            answer_start, answer_end positions, answer logits which represent models confidence\n",
    "        \"\"\"\n",
    "        if any(np.sum(c_tokens, axis=-1) == 0) or any(np.sum(q_tokens, axis=-1) == 0):\n",
    "            logger.info('SQuAD model: Warning! Empty question or context was found.')\n",
    "            noanswers = -np.ones(shape=(c_tokens.shape[0]), dtype=np.int32)\n",
    "            return noanswers, noanswers\n",
    "\n",
    "        feed_dict = self._build_feed_dict(c_tokens, c_chars, q_tokens, q_chars)\n",
    "        yp1, yp2, logits = self.sess.run([self.yp1, self.yp2, self.yp_logits], feed_dict=feed_dict)\n",
    "        return yp1, yp2, [float(logit) for logit in logits]\n",
    "\n",
    "    def process_event(self, event_name: str, data) -> None:\n",
    "        \"\"\"\n",
    "        Processes events sent by trainer. Implements learning rate decay.\n",
    "\n",
    "        Args:\n",
    "            event_name: event_name sent by trainer\n",
    "            data: number of examples, epochs, metrics sent by trainer\n",
    "        \"\"\"\n",
    "        if event_name == \"after_validation\":\n",
    "            if data['impatience'] > self.last_impatience:\n",
    "                self.lr_impatience += 1\n",
    "            else:\n",
    "                self.lr_impatience = 0\n",
    "\n",
    "            self.last_impatience = data['impatience']\n",
    "\n",
    "            if self.lr_impatience >= self.learning_rate_patience:\n",
    "                self.lr_impatience = 0\n",
    "                self.learning_rate = max(self.learning_rate / 2, self.min_learning_rate)\n",
    "                logger.info('SQuAD model: learning_rate changed to {}'.format(self.learning_rate))\n",
    "            logger.info('SQuAD model: lr_impatience: {}, learning_rate: {}'.format(self.lr_impatience, self.learning_rate))\n",
    "\n",
    "    def shutdown(self):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
